% --------------------------------------------------------------
% This is all preamble stuff that you don't have to worry about.
% Head down to where it says "Start here"
% --------------------------------------------------------------
 
\documentclass[12pt]{article}
 
\usepackage[margin=1in]{geometry} 
\usepackage{amsmath,amsthm,amssymb}
 
\newcommand{\N}{\mathbb{N}}
\newcommand{\Z}{\mathbb{Z}}
 
\newenvironment{theorem}[2][Theorem]{\begin{trivlist}
\item[\hskip \labelsep {\bfseries #1}\hskip \labelsep {\bfseries #2.}]}{\end{trivlist}}

\newenvironment{lemma}[2][Lemma]{\begin{trivlist}
\item[\hskip \labelsep {\bfseries #1}\hskip \labelsep {\bfseries #2.}]}{\end{trivlist}}

\newenvironment{exercise}[2][Exercise]{\begin{trivlist}
\item[\hskip \labelsep {\bfseries #1}\hskip \labelsep {\bfseries #2.}]}{\end{trivlist}}

\newenvironment{problem}[2][Problem]{\begin{trivlist}
\item[\hskip \labelsep {\bfseries #1}\hskip \labelsep {\bfseries #2}]}{\end{trivlist}}

\newenvironment{question}[2][Question]{\begin{trivlist}
\item[\hskip \labelsep {\bfseries #1}\hskip \labelsep {\bfseries #2.}]}{\end{trivlist}}

\newenvironment{corollary}[2][Corollary]{\begin{trivlist}
\item[\hskip \labelsep {\bfseries #1}\hskip \labelsep {\bfseries #2.}]}{\end{trivlist}}

\usepackage{graphicx}
\graphicspath{{./}}

\begin{document}
 
% --------------------------------------------------------------
%                         Start here
% --------------------------------------------------------------
 
\title{Homework 5}%replace X with the appropriate number
\author{Yunzhong He\\ %replace with your name
204010749} %if necessary, replace with your course title
 
\maketitle
 
\begin{problem}{1. Bias-Variance Tradeoff}
\def \yy {X^T\hat{\beta_\lambda}}
\def \ys {X^T\beta^*}

\item{(a)}
To minimize $\hat{\beta_\lambda}$ we have
\begin{align*}
	\frac{d (y-X\beta)'(y-X\beta)}{d \beta} = -2X'y + 2(X'X)^{-1}\beta + 2\lambda\beta = 0
\end{align*}
Thus
\begin{align*}
	\hat{\beta_\lambda} = (X'X - \lambda I)^{-1}X'y
\end{align*}
Plug in $y = \ys + \epsilon$ we have
\begin{align*}
	\hat{\beta_\lambda} = (X'X - \lambda I)^{-1}X'(X'\beta^* + \epsilon) = A(\lambda)b + A(\lambda)\epsilon
\end{align*}
where A = $(X'X - \lambda I)^{-1}X'$ and b = $X'\beta^*$
\\\\
\item{(b)} The bias is thus
\begin{align*}
	E[\yy] - \ys = A(\lambda)b + A(\lambda)E(\epsilon) - \ys
\end{align*}
\\
\item{(c)} The variance is thus
\begin{align*}
	E[(\yy - E(\yy))^2] = (X'A(\lambda))^2 var(\epsilon)
\end{align*}
\item{(d)} From (b) (c) and bias-variance theorem we can see that when $\lambda$ is large, bias dominates the squred errors, and when $\lambda$ is small, bias dominates squared error. 
\end{problem}

\begin{problem}{2. Kernalized Perception}
\item{(a)} Proof by induction:\\
(1) At iteration T = 0, we have $w = 0 = \sum_i^N 0\cdot x_i$, and is a linear combination of $x$.\\
(2) Assume at iteration T = k, $w$ is a linear combination of $x$, then at T = k+1 we have 
\begin{align*}
&w_{k+1} = w_k + \phi(x_{k+1}) \ if \ w'\phi(x) > 0\\
&w_{k+1} = w_k - \phi(x_{k+1}) \ otherwise
\end{align*}
In either case $w_{k+1}$ is a linear combination of $\phi(x)$, thus $w$ is always a linear combination of $\phi(x)$.
\\
\item{(b)} From the proof in (a) we can see that $w_k$ is actually a linear combination of $\phi(x_1), \phi(x_2), ..., \phi(x_k)$, and the prediction rule at T = k+1 can be written as
\begin{align*}
	y &= sign(w_k'\phi(x_{k+1})) = sign(\sum_i^{k} \alpha_i\phi(x_i)'\phi(x_{k+1})) \\
	  &= sign(\sum_i^{k} \alpha_i<\phi(x_i), \phi(x_{k+1})>)
\end{align*}
which only depends on the inner products of the mapping $\phi$.
\\
\item{(c)}
From (b) we know that the perception algorithm can be rewritten as follows.\\
(1) Initialize $\alpha_0 = 0$\\
(2) At iteration T = k+1, compute $y$ and update $w$ as follows\\
\indent val = 0\\
\indent For i from 1 to k\\
\indent \indent val = val + $\alpha_i <\phi(x_i), \phi(x_{k+1})>$\\
\indent end\\
\indent $y = sign(val)$\\
\indent $w = w + y\phi(x_{k+1})$\\
\end{problem}

\begin{problem}{3. Kernels}
\item{(a)}
Since $K_1$ $K_2$ are positive semi-definite, we have $x^TK_1x$ and $x^TK_2x$ being non-negative at every entry for any vector $x$. Therefore we have
\begin{align*}
x^TK_3x = x^T(a_1K_1 + a_2K_2)x = a_1x^TK_1x + a_2x^TK_2x
\end{align*}
Since $a_1 \geq 0$ and $a_2 \geq 0$, we know every entry in $x^TK_3x$ is non-negative as well. Thus $K_3$ is positive semi-definite, and by Mercer's theorm so is $k_3$.
\\
\item{(b)} 
For every entry $K_4(i, j)$ of $K_4$, we know
\begin{align*}
K_4(i, j) = f(x_i)f(x_j) = f(x_j)f(x_i) = K_4(j, i)
\end{align*}
Thus $K_4$ is a symmetric matrix and is therefore positive semi-definite, by Mercer's theorm so is $k_4$
\item{(c)}
For any vector $x$, suppose $u = x^TK_1x$ and $v = x^TK_2x$, we know every entries in $u$ and $v$ are non-negative. Also, for $w = x^TK_5x$, we know $x(i) = u(i)v(i)$ because $k_5(x, x') = k_1(x, x')k_2(x, x')$. Thus every entries in $w$ are also non-negative, and $K_5$ is positive semi-definite. By Mercer's theorm $k_5$ is postitive semi-definite.
\end{problem}

\begin{problem}{4. Soft Margin Hyperplanes}
\item{(a)} With more general form of soft margin $\xi_i^p$, the dual formulation of optimization objective is thus
\begin{align*}
	&min_{\xi^p, w, b} \frac{1}{2}||w||_2^2 + C\sum_n||\xi_n^p||_2^2 \ s.t.\\
	&\forall n \ y_n[w^T\phi(x_n)+b] \geq 1 - \xi_n^p, \ \ \xi_n^p \geq 0, \ \ p > 1
\end{align*}
\item{(b)}
This more general formulation allows extra degrees of freedom at different dimensions in the choice of $\xi$, so it is able to generate more flexible margins. However, each $\xi_n^p$ now is a p-dimentional vector rather than a real number, making the optimization objective more complex.
\end{problem}

\begin{problem}{5. Programming}
\item{5.1}
The 10th feature's mean is \textbf{2.527} and standard deviation is \textbf{1.1245}. We estimate the mean and standard deviation from training data because the goal is to normalize the training data, and it does not matter whether it is a good estimate to the real distribution or not.
\item{5.3 (a)}
The cross-validation accuracy for different C can be found below
\begin{center}
	C     \:\: accu\\
	$4^{-6}$ \: 0.659\\
	$4^{-5}$ \: 0.659\\
	$4^{-4}$ \: 0.659\\
	$4^{-3}$ \: 0.659\\
	$4^{-2}$ \: 0.673\\
	$4^{-1}$ \: 0.725\\
	$4^{-0}$ \: 0.802\\
	$4^1$ \:\:\:  0.837\\
	$4^2$ \:\:\:  0.841\\
\end{center}
\item{5.3 (b) (c)}
Based on 5.3 we shoud use \textbf{C = 16}, and the corresponding test accuracy is \textbf{0.852}
\item{5.4 (a) (b)}
Running linear SVM with libSVM with different C we obtained the following results, with average training time \textbf{1.13} seconds, which is around 10 times faster than my implementation in 5.3. This proves the efficiency of dual form SVM, but it could also due to some other performance optimizations that libSVM have. 
\begin{center}
\begin{tabular}{ l | r}
    C &  accuracy\\
    $4^{-6}$  & 51.70000\\
    $4^{-5}$  & 51.70000\\
    $4^{-4}$  & 51.70000\\
    $4^{-3}$  & 51.70000\\
    $4^{-2}$  & 58.40000\\
    $4^{-1}$  & 83.50000\\
    $4^{-0}$  & 87.20000\\
    $4^{1}$  & 88.50000\\
    $4^{2}$  & 88.20000\\
\end{tabular}
\end{center}

\item{5.5 (a)}
Running polynomial SVM using libSVM with different degree and C, we obtained the following 5-fold cross-validation accuracies, with average training time \textbf{9.24} seconds.
\begin{center}
\begin{tabular}{ l | c | r }
    degree &   C  &  accuracy\\
    1.00000 &  $4^{-3}$  & 51.70000\\
    1.00000 &  $4^{-2}$  & 78.70000\\
    1.00000 &  $4^{-1}$  & 80.40000\\
    1.00000  &  $4^{0}$  & 79.50000\\
    1.00000  &  $4^{1}$ & 79.20000\\
    1.00000  &  $4^{2}$ & 79.50000\\
    1.00000  &  $4^{3}$ & 79.40000\\
    1.00000  &  $4^{4}$ & 79.30000\\
    1.00000  &  $4^{5}$ & 79.20000\\
    1.00000  &  $4^{6}$ &79.30000\\
    1.00000  &  $4^{7}$ & 78.80000\\
    2.00000  & $4^{-3}$ & 51.70000\\
    2.00000  & $4^{-2}$ & 51.70000\\
    2.00000  & $4^{-1}$ & 65.40000\\
    2.00000  &  $4^{0}$ & 72.80000\\
    2.00000  &  $4^{1}$ &70.90000\\
\end{tabular}
\begin{tabular}{ l | c | r }
    2.00000  &  $4^{2}$ & 70.90000\\
    2.00000  &  $4^{3}$ & 70.90000\\
    2.00000  &  $4^{4}$ & 70.90000\\
    2.00000  &  $4^{5}$ & 70.90000\\
    2.00000  &  $4^{6}$  &70.90000\\
    2.00000  &  $4^{7}$  &70.90000\\
    3.00000  & $4^{-3}$  &51.70000\\
    3.00000  & $4^{-2}$  &51.70000\\
    3.00000  & $4^{-1}$ & 62.10000\\
    3.00000  &  $4^{0}$  &79.20000\\
    3.00000  &  $4^{1}$ & 79.00000\\
    3.00000  &  $4^{2}$  &79.20000\\
    3.00000  &  $4^{3}$  &79.20000\\
    3.00000  &  $4^{4}$  &79.20000\\
    3.00000  &  $4^{5}$  &79.20000\\
    3.00000  &  $4^{6}$  &79.20000\\
    3.00000  &  $4^{7}$ & 79.20000\\
\end{tabular}
\end{center}
\end{problem}
% --------------------------------------------------------------
%     You don't have to mess with anything below this line.
% --------------------------------------------------------------
 
\end{document}
